{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9740d8be",
   "metadata": {},
   "source": [
    "# 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ffb02",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial step in natural language processing (NLP) and text analysis. Its purpose is to clean and transform raw text data into a format that is suitable for analysis, making it easier for machine learning algorithms and other analytical tools to extract meaningful information. Here are some key reasons why text preprocessing is essential:\n",
    "\n",
    "1. **Noise Reduction:** Raw text data often contains irrelevant information, such as special characters, numbers, and punctuation, which may not contribute to the analysis. Text preprocessing helps remove this noise to focus on the relevant content.\n",
    "\n",
    "2. **Tokenization:** Breaking down text into smaller units, such as words or phrases (tokens), facilitates further analysis. Tokenization is a fundamental step in text preprocessing that aids in understanding the structure of the text.\n",
    "\n",
    "3. **Normalization:** Standardizing the text by converting it to lowercase helps ensure uniformity and consistency. This is important, especially when dealing with case-insensitive tasks like text comparison or sentiment analysis.\n",
    "\n",
    "4. **Lemmatization and Stemming:** These techniques reduce words to their base or root form, helping to group variations of words together. This is beneficial for tasks like information retrieval and text mining.\n",
    "\n",
    "5. **Stopword Removal:** Stopwords are common words (e.g., \"and,\" \"the,\" \"is\") that often do not carry significant meaning. Removing stopwords can improve the efficiency of text analysis by focusing on more meaningful terms.\n",
    "\n",
    "6. **Handling Missing Data:** Dealing with missing or incomplete text data is essential for a comprehensive analysis. Text preprocessing may involve strategies such as filling in missing values or excluding incomplete entries.\n",
    "\n",
    "7. **Feature Engineering:** Transforming text data into numerical features is essential for machine learning models. Techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings convert text into a format suitable for modeling.\n",
    "\n",
    "8. **Handling Special Characters and Encoding:** Text data may contain special characters or need encoding to handle different character sets. Preprocessing addresses these issues to ensure the data is ready for analysis.\n",
    "\n",
    "In summary, text preprocessing is crucial before analysis because it enhances the quality of the data, removes unnecessary elements, and transforms text into a structured format that facilitates effective analysis and modeling in the field of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97797b14",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670d83b",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into smaller units, often words or phrases, known as tokens. In the context of natural language processing (NLP), tokenization is a fundamental step in text processing and analysis. The significance of tokenization lies in its ability to structure and organize raw text data, making it suitable for further analysis. Here are some key aspects of tokenization and its significance:\n",
    "\n",
    "1. **Breaking Text into Units:**\n",
    "   - **Word Tokenization:** In word tokenization, the text is segmented into individual words. This is a common approach and is suitable for many NLP tasks.\n",
    "   - **Sentence Tokenization:** In sentence tokenization, the text is segmented into sentences. This is useful for tasks that require a sentence-level analysis.\n",
    "\n",
    "2. **Facilitating Analysis:**\n",
    "   - Tokenization helps in understanding the structure of the text by breaking it down into meaningful units. This is essential for various NLP applications, such as sentiment analysis, part-of-speech tagging, and named entity recognition.\n",
    "\n",
    "3. **Preparation for Further Processing:**\n",
    "   - Once the text is tokenized, it can be further processed and analyzed. Each token becomes a unit of information that can be examined, transformed, or used as a feature in machine learning models.\n",
    "\n",
    "4. **Feature Extraction:**\n",
    "   - Tokenization is a crucial step in feature extraction for NLP tasks. Many natural language processing models require numerical input, and tokenization is often followed by techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings to represent words in a numerical format.\n",
    "\n",
    "5. **Language Understanding:**\n",
    "   - Tokenization is essential for language understanding as it breaks down the text into elements that can be analyzed and interpreted. This is particularly important for tasks like sentiment analysis, where the sentiment may be associated with specific words or phrases.\n",
    "\n",
    "6. **Text Comparison and Retrieval:**\n",
    "   - Tokenization is useful in tasks that involve text comparison and retrieval. It enables the comparison of individual words or phrases, making it easier to identify similarities and differences between texts.\n",
    "\n",
    "7. **Improved Efficiency:**\n",
    "   - Breaking text into tokens reduces the complexity of the data and makes it more manageable. This can lead to more efficient processing and analysis, especially in the context of large datasets.\n",
    "\n",
    "8. **Handling Ambiguity:**\n",
    "   - Tokenization can help in handling ambiguity in natural language. By breaking down the text into smaller units, the ambiguity associated with certain phrases or expressions can be reduced, making it easier to analyze.\n",
    "\n",
    "In summary, tokenization is a critical step in NLP that involves breaking down text into smaller units to facilitate analysis and processing. It plays a key role in various NLP applications by providing a structured representation of text data for further exploration and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516a673",
   "metadata": {},
   "source": [
    "# Breaking Text into Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe818cf0",
   "metadata": {},
   "source": [
    "# Sentence Tokenization - spliting as a sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585173eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "====================================================================================================\n",
      "Hello Mr.Starc, i hope you doing good.\n",
      "By the way I have a plan to visit to your house in the next week of the month.\n",
      "We have a big business plan to discuss\n",
      "====================================================================================================\n",
      "\n",
      "After Sentence Tokenization:\n",
      " ['Hello Mr.Starc, i hope you doing good.', 'By the way I have a plan to visit to your house in the next week of the month.', 'We have a big business plan to discuss']\n",
      "====================================================================================================\n",
      "No.of Sentences:\t 3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"Hello Mr.Starc, i hope you doing good.\n",
    "By the way I have a plan to visit to your house in the next week of the month.\n",
    "We have a big business plan to discuss\"\"\"\n",
    "\n",
    "print('Original text:')\n",
    "print('='*100)\n",
    "print(text)\n",
    "print('='*100)\n",
    "print()\n",
    "tokenised_sent=sent_tokenize(text)\n",
    "print('After Sentence Tokenization:\\n',tokenised_sent)\n",
    "print('='*100)\n",
    "print('No.of Sentences:\\t',len(tokenised_sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7175f3",
   "metadata": {},
   "source": [
    "# Word Tokenization - Spliting as a words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d418af30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "====================================================================================================\n",
      "Hello Mr.Starc, i hope you doing good.\n",
      "By the way I have a plan to visit to your house in the next week of the month.\n",
      "We have a big business plan to discuss\n",
      "====================================================================================================\n",
      "\n",
      "After word Tokenization:\n",
      " ['Hello', 'Mr.Starc', ',', 'i', 'hope', 'you', 'doing', 'good', '.', 'By', 'the', 'way', 'I', 'have', 'a', 'plan', 'to', 'visit', 'to', 'your', 'house', 'in', 'the', 'next', 'week', 'of', 'the', 'month', '.', 'We', 'have', 'a', 'big', 'business', 'plan', 'to', 'discuss']\n",
      "====================================================================================================\n",
      "No.of words:\t 37\n",
      "====================================================================================================\n",
      "word :  Hello & Length : 5\n",
      "word :  Mr.Starc & Length : 8\n",
      "word :  , & Length : 1\n",
      "word :  i & Length : 1\n",
      "word :  hope & Length : 4\n",
      "word :  you & Length : 3\n",
      "word :  doing & Length : 5\n",
      "word :  good & Length : 4\n",
      "word :  . & Length : 1\n",
      "word :  By & Length : 2\n",
      "word :  the & Length : 3\n",
      "word :  way & Length : 3\n",
      "word :  I & Length : 1\n",
      "word :  have & Length : 4\n",
      "word :  a & Length : 1\n",
      "word :  plan & Length : 4\n",
      "word :  to & Length : 2\n",
      "word :  visit & Length : 5\n",
      "word :  to & Length : 2\n",
      "word :  your & Length : 4\n",
      "word :  house & Length : 5\n",
      "word :  in & Length : 2\n",
      "word :  the & Length : 3\n",
      "word :  next & Length : 4\n",
      "word :  week & Length : 4\n",
      "word :  of & Length : 2\n",
      "word :  the & Length : 3\n",
      "word :  month & Length : 5\n",
      "word :  . & Length : 1\n",
      "word :  We & Length : 2\n",
      "word :  have & Length : 4\n",
      "word :  a & Length : 1\n",
      "word :  big & Length : 3\n",
      "word :  business & Length : 8\n",
      "word :  plan & Length : 4\n",
      "word :  to & Length : 2\n",
      "word :  discuss & Length : 7\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print('Original text:')\n",
    "print('='*100)\n",
    "print(text)\n",
    "print('='*100)\n",
    "print()\n",
    "\n",
    "token_word=word_tokenize(text)\n",
    "\n",
    "print('After word Tokenization:\\n',token_word)\n",
    "print('='*100)\n",
    "print('No.of words:\\t',len(token_word))\n",
    "print('='*100)\n",
    "\n",
    "for i in token_word:\n",
    "    print('word : ',i,'&','Length :',len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfecdb39",
   "metadata": {},
   "source": [
    "# 3. Difference between stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd955dcb",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their base or root forms. The key differences are:\n",
    "\n",
    "- **Stemming:** Removes prefixes or suffixes to obtain a word's root form, but the result may not be an actual word. It is a faster and less accurate method.\n",
    "\n",
    "- **Lemmatization:** Involves reducing words to their base or dictionary form (lemma), ensuring the result is a valid word. It is a more accurate but computationally intensive process.\n",
    "\n",
    "Choose stemming for efficiency in information retrieval or search engine applications. Choose lemmatization for tasks requiring linguistic precision, such as sentiment analysis or machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b568cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filtered words:\n",
      "\n",
      "\t ['connect', ',', 'connected', ',', 'connecting']\n",
      "====================================================================================================\n",
      "Length after Stemmer:\t 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " After Stemming- words are:\n",
      "\t ['connect', ',', 'connect', ',', 'connect']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "st='connect,connected,connecting'\n",
    "st_words=word_tokenize(st)\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "stemmed_words=[]\n",
    "for w in st_words:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    " \n",
    "print('Length of words:\\t',len(st_words))\n",
    "print('-'*100)\n",
    "print('Filtered words:\\n\\n\\t',st_words)\n",
    "print('='*100)\n",
    "print('Length after Stemmer:\\t',len(stemmed_words))\n",
    "print('-'*100)\n",
    "print('\\n After Stemming- words are:\\n\\t',stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff20cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentences:\n",
      "\n",
      "\tconnected connecting flying swimming seems\n",
      "====================================================================================================\n",
      "Tokens of word:\n",
      " ['connected', 'connecting', 'flying', 'swimming', 'seems']\n",
      "Length: \t 5\n",
      "****************************************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original text:\n",
      " connected connecting flying swimming seems\n",
      "====================================================================================================\n",
      "Lemmatized words:\n",
      " ['connect', 'connect', 'fly', 'swim', 'seem']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Initialize the wordnet lemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "#prompt to take input string\n",
    "text=input('Enter a sentences:\\n\\n\\t')\n",
    "\n",
    "#split as word tokens\n",
    "words=word_tokenize(text)\n",
    "print('='*100)\n",
    "print('Tokens of word:\\n',words)\n",
    "print('Length: \\t',len(words))\n",
    "print('*'*100)\n",
    "\n",
    "#Apply Lemmatization to each word\n",
    "\n",
    "lemma_words=[lemma.lemmatize(word,pos='v') for word in words ]\n",
    "\n",
    "print('-'*100)\n",
    "print('Original text:\\n',text)\n",
    "print('='*100)\n",
    "print('Lemmatized words:\\n',lemma_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7a644",
   "metadata": {},
   "source": [
    "# 4. StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b5750",
   "metadata": {},
   "source": [
    "# **Stopwords** are common words, such as \"and,\" \"the,\" \"is,\" etc., that are often removed during text preprocessing in natural language processing (NLP). They play a crucial role in text preprocessing by:\n",
    "\n",
    "1. **Noise Reduction:** Removing stopwords helps eliminate frequently occurring but less meaningful words, reducing noise in the data.\n",
    "\n",
    "2. **Improved Efficiency:** By excluding stopwords, NLP tasks become more computationally efficient as the focus shifts to more meaningful words.\n",
    "\n",
    "3. **Enhancing Analysis:** Stopword removal allows algorithms to focus on content-carrying words, improving the accuracy of tasks like sentiment analysis, text classification, and information retrieval.\n",
    "\n",
    "In summary, stopwords impact NLP tasks positively by enhancing the efficiency and accuracy of text analysis through the removal of common but less informative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adcfcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No.of Stopwords:\t 179\n",
      "====================================================================================================\n",
      "The Stop words are:\n",
      "\n",
      "{'any', 'll', \"it's\", 'of', 'he', \"hasn't\", \"weren't\", \"you'll\", 'into', 'for', 'yourselves', 'own', 'at', 'all', 'wasn', 'isn', 'too', 'some', 'same', 'was', 'when', 'nor', 'herself', 'further', 'to', 'where', 'm', 'can', \"aren't\", 'below', 'am', 'before', 'just', 'ours', 'being', 'his', 'each', 'as', 'its', 'because', 'aren', \"shouldn't\", \"needn't\", \"hadn't\", 'i', 'the', \"she's\", \"shan't\", 'her', \"wasn't\", 'be', 'yourself', 'above', 's', 'from', 'in', 'if', 'do', 'yours', 'your', 'does', \"couldn't\", \"haven't\", 'doing', 'up', 'after', 'under', 'shan', \"you'd\", 'having', 'a', 'myself', 'then', 'so', \"don't\", \"isn't\", 'with', 'out', 'again', 'mustn', 'an', 'himself', 'this', 'by', 'theirs', \"should've\", 'my', 'only', 'those', 'until', 'have', 'ma', 'what', 'o', 'over', 'why', 'most', 'not', 'hers', 'them', 'these', 'don', 'are', 'weren', 'down', 'were', 'we', 't', 'had', 'whom', 've', \"didn't\", 'who', 'during', 'is', 'between', 'mightn', 'me', \"you're\", 'has', 'through', 'y', 'or', 'she', 'doesn', 'more', 'such', 'should', 'been', 'needn', 'hasn', 'than', 'themselves', 'that', 'their', 'it', \"doesn't\", \"mustn't\", 'wouldn', 'how', 'ain', 'they', 'there', 'did', \"wouldn't\", 'against', 'which', 'while', 'but', 'about', 'won', 'very', 'couldn', 'both', 'and', 'will', 'on', 'd', 'few', 'ourselves', 'now', 'no', \"you've\", 'didn', 'him', 're', 'hadn', 'other', 'shouldn', 'you', \"that'll\", 'haven', 'here', \"won't\", 'once', 'itself', 'our', 'off', \"mightn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "print('Total No.of Stopwords:\\t',len(stop_words))\n",
    "print('='*100)\n",
    "print('The Stop words are:\\n')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13e3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tokenized words-with stopwords:\n",
      "\n",
      "\t ['Hello', 'Mr.Starc', ',', 'i', 'hope', 'you', 'doing', 'good', '.', 'By', 'the', 'way', 'I', 'have', 'a', 'plan', 'to', 'visit', 'to', 'your', 'house', 'in', 'the', 'next', 'week', 'of', 'the', 'month', '.', 'We', 'have', 'a', 'big', 'business', 'plan', 'to', 'discuss']\n",
      "====================================================================================================\n",
      "Length after Remove stopwords:\t 21\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " After Removing stopwords- words are:\n",
      "\t ['Hello', 'Mr.Starc', ',', 'hope', 'good', '.', 'By', 'way', 'I', 'plan', 'visit', 'house', 'next', 'week', 'month', '.', 'We', 'big', 'business', 'plan', 'discuss']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens=[]\n",
    "for w in token_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "print('Length of words:\\t',len(token_word))\n",
    "print('-'*100)\n",
    "print('Tokenized words-with stopwords:\\n\\n\\t',token_word)\n",
    "print('='*100)\n",
    "print('Length after Remove stopwords:\\t',len(filtered_tokens))\n",
    "print('-'*100)\n",
    "print('\\n After Removing stopwords- words are:\\n\\t',filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0a357",
   "metadata": {},
   "source": [
    "# 5. Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206b770",
   "metadata": {},
   "source": [
    "**Removing punctuation** in text preprocessing in NLP contributes by:\n",
    "\n",
    "1. **Noise Reduction:** Eliminating punctuation helps reduce unnecessary characters that might not contribute to the meaning of the text, reducing noise in the data.\n",
    "\n",
    "2. **Facilitating Tokenization:** Punctuation removal aids in breaking down the text into meaningful units (tokens), making subsequent tokenization processes more effective.\n",
    "\n",
    "3. **Improved Analysis:** Punctuation removal is essential for tasks like sentiment analysis and part-of-speech tagging, where the presence or absence of punctuation can influence the meaning of a text.\n",
    "\n",
    "In summary, removing punctuation is beneficial for noise reduction, tokenization, and improving the accuracy of various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb92669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentences:\n",
      "\n",
      "\tHello Mr.Starc, i hope you doing good. By the way I have a plan to visit to your house in the next week of the month. We have a big business plan to discuss\n",
      "====================================================================================================\n",
      "Length: \t 37\n",
      "Tokens of word:\n",
      " ['Hello', 'Mr.Starc', ',', 'i', 'hope', 'you', 'doing', 'good', '.', 'By', 'the', 'way', 'I', 'have', 'a', 'plan', 'to', 'visit', 'to', 'your', 'house', 'in', 'the', 'next', 'week', 'of', 'the', 'month', '.', 'We', 'have', 'a', 'big', 'business', 'plan', 'to', 'discuss']\n",
      "****************************************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original text:\n",
      " Hello Mr.Starc, i hope you doing good. By the way I have a plan to visit to your house in the next week of the month. We have a big business plan to discuss\n",
      "====================================================================================================\n",
      "After Removing Punctuation:\n",
      "\n",
      "['Hello', 'i', 'hope', 'you', 'doing', 'good', 'By', 'the', 'way', 'I', 'have', 'a', 'plan', 'to', 'visit', 'to', 'your', 'house', 'in', 'the', 'next', 'week', 'of', 'the', 'month', 'We', 'have', 'a', 'big', 'business', 'plan', 'to', 'discuss'] 33\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "#prompt to take input string\n",
    "text=input('Enter a sentences:\\n\\n\\t')\n",
    "\n",
    "#split as word tokens\n",
    "tokens=word_tokenize(text)\n",
    "print('='*100)\n",
    "print('Length: \\t',len(tokens))\n",
    "print('Tokens of word:\\n',tokens)\n",
    "print('*'*100)\n",
    "\n",
    "#Remove all tokens that are not alphabetic\n",
    "words=[word for word in tokens if word.isalpha()]\n",
    "\n",
    "print('-'*100)\n",
    "print('Original text:\\n',text)\n",
    "print('='*100)\n",
    "print('After Removing Punctuation:\\n')\n",
    "print(words,len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e67d4f",
   "metadata": {},
   "source": [
    "# 6. Lowercase Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6a2ca",
   "metadata": {},
   "source": [
    "**Lowercase conversion** is a common step in text preprocessing for NLP tasks due to its importance in:\n",
    "\n",
    "1. **Consistency:** Converting text to lowercase ensures uniformity, making it easier to compare and analyze words without being case-sensitive.\n",
    "\n",
    "2. **Normalization:** It helps in standardizing the text, reducing the complexity of variations arising from different letter cases.\n",
    "\n",
    "3. **Efficient Matching:** Lowercasing facilitates efficient matching of words, improving the accuracy of tasks such as information retrieval and text classification.\n",
    "\n",
    "4. **Feature Extraction:** Many NLP models and algorithms rely on the numerical representation of text features. Lowercasing ensures consistency in feature extraction.\n",
    "\n",
    "5. **Enhanced Generalization:** Lowercasing enables models to generalize better by treating words with different cases as the same entity, improving performance across diverse text inputs.\n",
    "\n",
    "In summary, lowercase conversion in text preprocessing is crucial for ensuring consistency, simplifying text analysis, and supporting efficient feature extraction in various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c719493",
   "metadata": {},
   "source": [
    "# 7. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43cbfd",
   "metadata": {},
   "source": [
    "**Vectorization** in the context of text data refers to the process of converting textual information into numerical vectors that can be used as input for machine learning algorithms. In natural language processing (NLP), vectorization is a crucial step because most machine learning models require numerical input, while text data is inherently non-numeric.\n",
    "\n",
    "**CountVectorizer** is a common technique for vectorization in NLP. It works by converting a collection of text documents to a matrix of token counts. Here's how CountVectorizer contributes to text preprocessing:\n",
    "\n",
    "1. **Word Frequency Representation:** CountVectorizer represents each document as a vector, where each element corresponds to the count of a particular word in the document. This captures the frequency of words and their distribution in the corpus.\n",
    "\n",
    "2. **Sparse Matrix:** The output of CountVectorizer is typically a sparse matrix, where most entries are zero because not every word appears in every document. This sparse representation is memory-efficient.\n",
    "\n",
    "3. **Feature Extraction:** CountVectorizer extracts features from text data, allowing machine learning models to work with the information. These features can be used for tasks like text classification, clustering, and information retrieval.\n",
    "\n",
    "4. **Handling Stopwords:** CountVectorizer can be configured to exclude common stopwords during the vectorization process. This helps in focusing on more meaningful terms and reduces the impact of common, less informative words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1e55ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Count Vectorizer Output:\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "\n",
    "# Create the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visibility\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Print the feature names and the resulting matrix\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"Count Vectorizer Output:\\n\", dense_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716fd54",
   "metadata": {},
   "source": [
    "# 8. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008b255",
   "metadata": {},
   "source": [
    "Normalization in the context of NLP involves transforming text data to a standard or normalized form. This step is essential to ensure uniformity and consistency in the representation of words, making it easier to analyze and compare text. Here are some common normalization techniques used in text preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a6669",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d16ca6",
   "metadata": {},
   "source": [
    "1. Lowercasing:\n",
    "\n",
    "Description: Converting all letters in the text to lowercase.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd11eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example text.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an Example Text.\"\n",
    "normalized_text = text.lower()\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031dd28",
   "metadata": {},
   "source": [
    "2. Stemming:\n",
    "\n",
    "Description: Reducing words to their base or root form by removing prefixes or suffixes.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b0dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word = \"running\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc78846",
   "metadata": {},
   "source": [
    "3. Lemmatization:\n",
    "\n",
    "Description: Reducing words to their base or dictionary form (lemma) to ensure they are valid words.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5414d74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemmatized_word = lemmatizer.lemmatize(word, pos='v')  # 'v' indicates the part of speech (verb)\n",
    "print(lemmatized_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80820e8c",
   "metadata": {},
   "source": [
    "4. Removing Special Characters:\n",
    "\n",
    "Description: Eliminating non-alphabetic characters, digits, or symbols.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636f9e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence with  numbers\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is an example sentence with 123 numbers!\"\n",
    "normalized_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ff33d",
   "metadata": {},
   "source": [
    "5. Removing Stopwords:\n",
    "\n",
    "Description: Eliminating common words that typically do not contribute much to the meaning of the text.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f357147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'common', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is an example sentence with some common words.\"\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda52662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
